# -*- coding: utf-8 -*-
"""code_23113167.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HN7Mpx-5znUwyslHJYRjMvhLuiGa4V06

<span style="font-size:30px">**Credit Card Behaviour Score Prediction Using Classification and Risk-Based Techniques**</span>

**visheshta 23113167**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score
import time

"""### Loading and understanding the data :"""

training_data = pd.read_csv('train_dataset_final1.csv')
testing_data = pd.read_csv('validate_dataset_final.csv')

training_data.head()

testing_data.head()

training_data.shape

testing_data.shape

training_data.isnull().sum()

testing_data.isnull().sum()

training_data['age'].fillna(training_data['age'].median , inplace = True)

training_data.isnull().sum()

training_data.dtypes

training_data['marriage'] = training_data['marriage'].astype('category')
training_data['sex'] = training_data['sex'].astype('category')
training_data['education'] = training_data['education'].astype('category')

training_data.info()

"""### **Exploratory Data Analysis (EDA)** :"""

plt.figure(figsize=(8,3))
labels = ['No Default (0)', 'Default (1)']

sns.countplot(x='next_month_default', data= training_data , color = 'lightgreen' , label = labels)
plt.title('Default Status')
plt.xlabel('Default Status (1 = Default)')
plt.ylabel('Count')
plt.show()

default_counts = training_data['next_month_default'].value_counts()
labels = ['No Default (0)', 'Default (1)']
colors = ['#05316b', '#5c0000']

plt.figure(figsize=(3, 3), )
plt.pie(default_counts, labels=labels, autopct='%1.1f%%', startangle=140, colors='green', explode=(0, 0.1))
plt.title('Percentage of Defaults')
plt.axis('equal')
plt.show()

training_data['age'] = pd.to_numeric(training_data['age'], errors='coerce')
bins = [20, 30, 40, 50, 60, 70, 80]
labels = ['20-30', '30-40', '40-50', '50-60', '60-70', '70-80']
training_data['AGE_GROUP'] = pd.cut(training_data['age'], bins=bins, labels=labels)
plt.figure(figsize=(10,3))
sns.countplot(x='AGE_GROUP', hue='next_month_default', data=training_data , palette={0: 'green', 1: 'Yellow'})
plt.title('Default Status by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.show()

education_labels = {
    1: 'Graduate School',
    2: 'University',
    3: 'High School',
    4: 'Others'
}
training_data['education_label'] = training_data['education'].map(education_labels)

plt.figure(figsize=(10, 3))
sns.countplot(x='education_label', hue='next_month_default', data = training_data,palette={0: 'green', 1: 'Yellow'})
plt.title('Default Rates by Education Level')
plt.xlabel('Education Level')
plt.ylabel('Count')
plt.legend(title='Default Status')
plt.show()

sex_labels = {
    0: 'Female',
    1: 'Male'
}
training_data['sex_label'] = training_data['sex'].map(sex_labels)

plt.figure(figsize=(10, 3))
sns.countplot(x='sex_label', hue='next_month_default', data=training_data,palette={0: 'green', 1: 'Yellow'})
plt.title('Default Rates by sex')
plt.xlabel('sex')
plt.ylabel('Count')
plt.legend(title='Default Status')
plt.show()

Marriage_status = {
    1: 'Married',
    2: 'Single',
    3: 'Others'
    }
training_data['Marriage_status'] = training_data['marriage'].map(Marriage_status)

plt.figure(figsize=(10, 3))
sns.countplot(x='Marriage_status', hue='next_month_default', data=training_data , palette={0: 'green', 1: 'Yellow'})
plt.title('Default Rates by marriage')
plt.xlabel('Marriage')
plt.ylabel('Count')
plt.legend(title='Default Status')
plt.show()

sns.set(style="darkgrid")
plt.rcParams['figure.figsize'] = (10, 3)

#pay_1 is not present in data
categorical_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']

fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 12))
axes = axes.flatten()

custom_palette = sns.color_palette("Set3", 10)

for i, col in enumerate(categorical_cols):
    unique_vals = sorted(training_data[col].dropna().unique())
    palette_subset = {val: custom_palette[j % len(custom_palette)] for j, val in enumerate(unique_vals)}

    sns.countplot(data=training_data,x=col,hue=col,palette=palette_subset,ax=axes[i],dodge=False)
    axes[i].set_title(f'Distribution of {col}', fontsize=20)
    axes[i].set_xlabel(col, fontsize=15)
    axes[i].set_ylabel("Count", fontsize=15)
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].legend().remove()

plt.tight_layout()
plt.suptitle("Distributions of PAY_* Columns", fontsize=18, y=1.02)
plt.show()

sns.set(style="darkgrid")

numerical_cols = ['Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6']


fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 10))
axes = axes.flatten()

for i, col in enumerate(numerical_cols[:6]):
    palette = ['yellow' , 'red']
    sns.histplot(data=training_data, x=col, hue='next_month_default', bins=50, kde=True, palette=palette, ax=axes[i])
    axes[i].set_title(f'Distribution of {col} by Default Status')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Count')

plt.tight_layout()
plt.show()

sns.set(style="darkgrid")

numerical_cols = ['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']


fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 10))
axes = axes.flatten()

for i, col in enumerate(numerical_cols[:6]):
    palette = ['yellow' , 'red']
    sns.histplot(data=training_data, x=col, hue='next_month_default', bins=50,kde = True , palette=palette, ax=axes[i])
    axes[i].set_title(f'Distribution of {col} by Default Status')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Count')

plt.tight_layout()
plt.show()

sns.set(style="darkgrid")

numerical_cols = ['LIMIT_BAL', 'age', 'AVG_Bill_amt', 'PAY_TO_BILL_ratio']


fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))
axes = axes.flatten()

for i, col in enumerate(numerical_cols[:4]):
    palette = ['yellow' , 'red']
    sns.histplot(data=training_data, x=col, hue='next_month_default', bins=50, kde=True, palette=palette, ax=axes[i])
    axes[i].set_title(f'Distribution of {col} by Default Status')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Count')

plt.tight_layout()
plt.show()

bill_cols = [f'Bill_amt{i}' for i in range(1, 7)]

bill_amt = training_data.groupby('next_month_default')[bill_cols].mean()

plt.figure(figsize=(10, 5))
for status in [0, 1]:
    plt.plot(bill_cols, bill_amt.loc[status],label=f'Default = {status}',marker='*')

plt.title('Average Bill Amount Trends by Default Status')
plt.xlabel('Months Before Default (BILL_AMT1 = Most Recent)')
plt.ylabel('Average Bill Amount')
plt.legend(title='Default Status')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### 3. Feature Engineering
*Create new features to enrich the dataset, including utilization ratio, repayment consistency, and others.*
"""

import math
pay_columns = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']
bill_amt_columns = ['Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6']
pay_amt_columns = ['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']

# Combine all columns to plot
all_columns = pay_columns + bill_amt_columns + pay_amt_columns

# Plot style settings
sns.set_theme(style="darkgrid")
plt.rcParams['axes.titlesize'] = 13
plt.rcParams['axes.labelsize'] = 11

# Calculate rows and columns for subplots
n_cols = 3
n_rows = math.ceil(len(all_columns) / n_cols)

# Create figure and axes
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(6 * n_cols, 5 * n_rows), constrained_layout=True)
axes = axes.flatten()

# Draw boxplots for each feature column
for idx, col in enumerate(all_columns):
    palette = ['yellow' , 'red']
    sns.boxplot(data=training_data, x='next_month_default', y=col, ax=axes[idx], palette=palette)
    axes[idx].set_title(f'{col} vs Default Status')
    axes[idx].set_xlabel("Default Next Month\n(0 = No, 1 = Yes)")
    axes[idx].set_ylabel(col)

# Hide any unused subplots
for ax in axes[len(all_columns):]:
    ax.set_visible(False)

# Add a main title
fig.suptitle("Boxplots of PAY_*, Bill_amt*, and pay_amt* Features by Default Status", fontsize=18, weight='bold')
plt.show()

"""**Feature Correlation Analysis**"""

numerical_features = ['LIMIT_BAL', 'age', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6', 'AVG_Bill_amt', 'PAY_TO_BILL_ratio']
plt.figure(figsize=(12, 10))
sns.heatmap(training_data[numerical_features].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

"""###  Feature Engineering in the data :

adding the following features : *feature name *utilization_ratio *delinquency_count *longest_delinquency_streak *repayment_consistency *overpayment_frequency *repay_ratio_1 to 6
"""

# Count of late payments (status > 0 means late)

pay_cols = ['pay_0' , 'pay_2' , 'pay_3' , 'pay_4' , 'pay_5' , 'pay_6']
training_data['total_late_payments'] = (training_data[pay_cols] > 0).sum(axis=1)

# Max delinquency severity
training_data['max_delinquency'] = training_data[pay_cols].max(axis=1)

# Current delinquency streak
def get_current_streak(row):
    streak = 0
    for i in pay_cols:  # Check PAY_0 first (most recent)
        if row[f'{i}'] > 0:
            streak += 1
        else:
            break
    return streak

training_data['current_delinquency_streak'] = training_data.apply(get_current_streak, axis=1)
training_data

# Count of months with only minimum payment (status = 0)
pay_cols = ['pay_0' , 'pay_2' , 'pay_3' , 'pay_4' , 'pay_5' , 'pay_6']
training_data['min_payment_months'] = (training_data[pay_cols] == 0).sum(axis=1)

# Count of months with full payment (status < 0)
training_data['full_payment_months'] = (training_data[pay_cols] < 0).sum(axis=1)

# Count of months with delayed payment (status > 0)
training_data['delayed_months'] = (training_data[pay_cols] > 0).sum(axis=1)

# Payment consistency
pay_amt_cols = [f'pay_amt{i}' for i in range(1, 7)]
training_data['payment_consistency'] = training_data[pay_amt_cols].std(axis=1)
training_data

def generate_features(df):
    df = df.copy()

    #Repayment Consistency
    repayment_cols = [f'pay_amt{i}' for i in range(1, 7)]
    df['Repayment_StdDev'] = df[repayment_cols].std(axis=1)

    #Monthly Repayment Ratios (Payment / Bill)
    for i in range(1, 7):
        pay_col = f'pay_amt{i}'
        bill_col = f'Bill_amt{i}'
        df[f'Repay_Ratio_{i}'] = df[pay_col] / df[bill_col].replace(0, np.nan)

    #Overpayment Frequency (Negative bills)
    bill_columns = [f'Bill_amt{i}' for i in range(1, 7)]
    df['Overpay_Count'] = df[bill_columns].lt(0).sum(axis=1)

    # Utilization Ratio (Avg Bill / Limit)
    df['Utilization_Rate'] = df['AVG_Bill_amt'] / df['LIMIT_BAL'].replace(0, np.nan)

    # Delinquency Features
    pay_status_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']
    df['Delinquency_Total'] = df[pay_status_cols].ge(1).sum(axis=1)
    df['Max_Delinquency'] = df[pay_status_cols].max(axis=1)

    return df

# apply to both datasets
training_data = generate_features(training_data)
testing_data = generate_features(testing_data)

for i in range(1, 7):
    training_data[f'UTILIZATION_{i}'] = training_data[f'Bill_amt{i}'] / training_data['LIMIT_BAL']

util_cols = [f'UTILIZATION_{i}' for i in range(1, 7)]
util = training_data.groupby('next_month_default')[util_cols].mean()

plt.figure(figsize=(12, 6))
for status in [0, 1]:
    plt.plot(util_cols, util.loc[status],
             label=f"Default = {status}",
             marker='o')
plt.title('Average Credit Utilization Trends')
plt.xlabel('Month')
plt.ylabel('Utilization Ratio')
plt.legend()
plt.grid()
plt.show()

"""###  data pre-processing"""

training_data_clean = training_data.drop(columns='Customer_ID', errors='ignore').copy()
testing_data_clean = testing_data.drop(columns='Customer_ID', errors='ignore').copy()

#use knn imputer for missing values
from sklearn.impute import SimpleImputer

numeric_cols = ['LIMIT_BAL', 'age', 'AVG_Bill_amt', 'PAY_TO_BILL_ratio', 'Utilization_Rate', 'Delinquency_Total', 'Max_Delinquency','Repayment_StdDev', 'Overpay_Count'
] + [f'pay_amt{i}' for i in range(1, 7)] + [f'Bill_amt{i}' for i in range(1, 7)] +  [f'Repay_Ratio_{i}' for i in range(1, 7)]

imputer = SimpleImputer(strategy='mean')

training_data_clean[numeric_cols] = imputer.fit_transform(training_data_clean[numeric_cols])
testing_data_clean[numeric_cols] = imputer.transform(testing_data_clean[numeric_cols])

# categorical columns to encode
categorical_cols = ['sex', 'education', 'marriage']

#Apply one-hot encoding to both train and validation datasets
training_data_encoded = pd.get_dummies(training_data_clean.copy(), columns=categorical_cols, drop_first=True)
testing_data_encoded = pd.get_dummies(testing_data_clean.copy(), columns=categorical_cols, drop_first=True)

training_data_encoded, testing_data_encoded = training_data_encoded.align(testing_data_encoded, join='left', axis=1, fill_value=0)

print("Train shape:", training_data_encoded.shape)
print("Testing shape:", testing_data_encoded.shape)

"""### Now both sheets have same no. of columns we can apply smote for sampling"""

training_data_encoded.drop(['AGE_GROUP', 'education_label', 'Marriage_status','sex_label'], axis=1, inplace=True)
training_data_encoded.info()

"""### Handling Class Imbalance
*Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance the target variable.*
"""

# Handling Class Imbalance since default came out to be 20%
# I will do it with SMOTE
from imblearn.over_sampling import SMOTE

X_train = training_data_encoded.drop(columns='next_month_default')
y_train = training_data_encoded['next_month_default']

# Initialize and fit the imputer on the training features (X_train)
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)

# Convert the imputed array back to a DataFrame to maintain column names
X_train = pd.DataFrame(X_train_imputed, columns=X_train.columns)

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Fit and resample using the imputed data
X_train_sampled, y_train_sampled = smote.fit_resample(X_train, y_train)

# Convert the resampled array back to a DataFrame to maintain column names
train_data_sampled = pd.DataFrame(X_train_sampled, columns = X_train_sampled.columns)
train_data_sampled['next_month_default'] = y_train_sampled

print(pd.Series(y_train_sampled).value_counts())

train_data_sampled.shape

"""### Now data is sampled , now apply different models and find which one is best and further do analysis aaccordingly

### 5) model building
"""

from sklearn.model_selection import train_test_split

# Separate features
X = train_data_sampled.drop('next_month_default', axis=1)
y = train_data_sampled['next_month_default']

# Split data into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Print shapes of the resulting DataFrames/Series
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import PowerTransformer

pipeline = Pipeline([
    ('yeojohnson', PowerTransformer(method='yeo-johnson')),
    ('clf', LogisticRegression(solver='liblinear', max_iter=1000))
])

pipeline.fit(X_train, y_train)

# Predict and evaluate
y_pred = pipeline.predict(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Initialize models
reg_model = LogisticRegression(solver='liblinear', max_iter=1000)
decision_tree_model = DecisionTreeClassifier()
xgboost_model = XGBClassifier()
lightgbm_model = LGBMClassifier()

# Train models
reg_model.fit(X_train, y_train)
decision_tree_model.fit(X_train, y_train)
xgboost_model.fit(X_train, y_train)
lightgbm_model.fit(X_train, y_train)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import f1_score, roc_auc_score, make_scorer
from sklearn.pipeline import Pipeline

# Define parameter grids for each model
param_dist_logreg = {
    'C': [ 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

param_dist_dt = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10,15],
    'min_samples_leaf': [1, 2, 4,6]
}

param_dist_xgb = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 0.9, 1.0]
}

param_dist_lgbm = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.05],
    'num_leaves': [31, 50],
    'max_depth': [-1, 5],
    'subsample': [0.8],
    'colsample_bytree': [0.8],
}

scoring = {'f1': make_scorer(f1_score), 'roc_auc': make_scorer(roc_auc_score)}
n_iter_search = 50

random_search_reg = RandomizedSearchCV(reg_model, param_distributions=param_dist_logreg, n_iter=n_iter_search, cv=5, scoring=scoring, refit='f1', random_state=42 ,  n_jobs=-1)
random_search_dt = RandomizedSearchCV(decision_tree_model, param_distributions=param_dist_dt, n_iter=n_iter_search, cv=5, scoring=scoring, refit='f1', random_state=42 , n_jobs=-1)
random_search_xgb = RandomizedSearchCV(xgboost_model, param_distributions=param_dist_xgb, n_iter=n_iter_search, cv=5, scoring=scoring, refit='f1', random_state=42, n_jobs=-1)
random_search_lgbm = RandomizedSearchCV(lightgbm_model, param_distributions=param_dist_lgbm, n_iter=n_iter_search, cv=5, scoring=scoring, refit='f1', random_state=42, n_jobs=-1)

# Fit RandomizedSearchCV to the training data
random_search_reg.fit(X_train, y_train)
random_search_dt.fit(X_train, y_train)
random_search_xgb.fit(X_train, y_train)
random_search_lgbm.fit(X_train, y_train)

# Store the best estimators
best_reg_model = random_search_reg.best_estimator_
best_dt_model = random_search_dt.best_estimator_
best_xgb_model = random_search_xgb.best_estimator_
best_lgbm_model = random_search_lgbm.best_estimator_

"""### Final Predictions and Submission
*Use the best model to generate predictions on the validation set for submission.*
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import pandas as pd

# Predictions from trained models
y_pred_reg = pipeline.predict(X_test)
y_pred_dt = best_dt_model.predict(X_test)
y_pred_xgb = best_xgb_model.predict(X_test)
y_pred_lgbm = best_lgbm_model.predict(X_test)

# Function to evaluate each model
def evaluate_model(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)
    roc_auc = roc_auc_score(y_true, y_pred)
    return [model_name, accuracy, precision, recall, f1, roc_auc]

# Evaluate all models
results = []
results.append(evaluate_model(y_test, y_pred_reg, "Logistic Regression (Yeo-Johnson)"))
results.append(evaluate_model(y_test, y_pred_dt, "Decision Tree"))
results.append(evaluate_model(y_test, y_pred_xgb, "XGBoost"))
results.append(evaluate_model(y_test, y_pred_lgbm, "LightGBM"))

# Create a summary table
results_df = pd.DataFrame(results, columns=["Model", "Accuracy", "Precision", "Recall", "F1-Score", "AUC-ROC"])
display(results_df)

"""### Threshold optimization"""

import numpy as np
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import fbeta_score

# Set beta for F2 score
beta = 2

# Dictionary of trained models with readable names
models = {
    "Logistic Regression": best_reg_model,
    "Decision Trees": best_dt_model,
    "XGBoost": best_xgb_model,
    "LightGBM": best_lgbm_model,
}

# Loop through each model
for name, model in models.items():
    print("="*60)
    print(f"ğŸ“Š Model: {name}")

    # Get predicted probabilities for class 1
    y_probs = model.predict_proba(X_test)[:, 1]

    # Compute precision, recall, and thresholds
    precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)

    # Compute F2 scores
    f2_scores = (1 + beta**2) * (precisions * recalls) / ((beta**2 * precisions) + recalls + 1e-8)

    # Get index of best F2
    best_idx = np.argmax(f2_scores)
    best_threshold = thresholds[best_idx]

    # Print the results
    print(f"ğŸ” Best Threshold (F2): {best_threshold:.4f}")
    print(f"ğŸ¯ Best F2 Score     : {f2_scores[best_idx]:.4f}")
    print(f"âœ… Precision @ F2    : {precisions[best_idx]:.4f}")
    print(f"ğŸ“ˆ Recall @ F2       : {recalls[best_idx]:.4f}")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Dictionary of trained models
models = {
    "Logistic Regression": best_reg_model,
    "Decision Trees": best_dt_model,
    "XGBoost": best_xgb_model,
    "LightGBM": best_lgbm_model,
}

# Loop through each model and plot its ROC curve separately
for name, model in models.items():
    # Get predicted probabilities for positive class (1)
    y_probs = model.predict_proba(X_test)[:, 1]

    # Compute ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test, y_probs)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.figure(figsize=(7, 5))
    plt.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.4f}')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Chance')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'AUC-ROC Curve - {name}')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

# Function to plot F2 Score vs Threshold for a single model
def plot_f2_vs_threshold(model, X_test, y_test, model_name="Model", beta=2):
    # Predict probabilities for class 1
    y_probs = model.predict_proba(X_test)[:, 1]

    # Precision-Recall-Thresholds
    precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)

    # F2 Score Calculation
    f2_scores = []
    for p, r in zip(precisions[:-1], recalls[:-1]):  # exclude last since thresholds is shorter
        if (beta**2 * p + r) == 0:
            f2_scores.append(0)
        else:
            f2 = (1 + beta**2) * p * r / ((beta**2 * p) + r)
            f2_scores.append(f2)

    f2_scores = np.array(f2_scores)

    # Best Threshold
    best_idx = np.argmax(f2_scores)
    best_threshold = thresholds[best_idx]
    best_f2 = f2_scores[best_idx]

    # Plot
    plt.figure(figsize=(8, 5))
    plt.plot(thresholds, f2_scores, label='F2 Score', color='blue')
    plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'Best Threshold = {best_threshold:.4f}')
    plt.scatter(best_threshold, best_f2, color='red')
    plt.title(f'F2 Score vs Threshold - {model_name}')
    plt.xlabel('Threshold')
    plt.ylabel('F2 Score')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    print(f"ğŸ“Š {model_name}")
    print(f"Best Threshold: {best_threshold:.4f}")
    print(f"Best F2 Score: {best_f2:.4f}")
    print("=" * 60)

# âœ… Models dictionary (replace with your trained model variables)
models = {
    "Logistic Regression": best_reg_model,
    "Decision Trees": best_dt_model,
    "XGBoost": best_xgb_model,
    "LightGBM": best_lgbm_model,
}

# âœ… Generate plots for all models
for name, model in models.items():
    plot_f2_vs_threshold(model, X_test, y_test, model_name=name)

OPTIMAL_F2_THRESHOLD = 0.1945
print("PART 1: PREDICTIONS ON VALIDATION DATASET (for submission)")

validate_df_processed = testing_data.copy()

# 1. Calculate credit utilization for validate_df
if 'Bill_amt1' in validate_df_processed.columns and 'LIMIT_BAL' in validate_df_processed.columns:
    validate_df_processed['LIMIT_BAL'] = validate_df_processed['LIMIT_BAL'].fillna(1)
    validate_df_processed['credit_utilization'] = np.where(
        validate_df_processed['LIMIT_BAL'] > 0,
        validate_df_processed['Bill_amt1'] / validate_df_processed['LIMIT_BAL'],
        0
    )
    print("âœ“ Credit utilization calculated for validate_df")
else:
    print("âš  Warning: Missing columns for credit utilization calculation")
    validate_df_processed['credit_utilization'] = 0

# Step 1: Align validate_df to training features
if 'X_train' in locals():
    expected_features = X_train.columns.tolist()

    # Ensure all expected features exist in validate_df
    missing_cols = [col for col in expected_features if col not in validate_df_processed.columns]
    for col in missing_cols:
        print(f"âš ï¸  Feature '{col}' missing in validate_df, adding with default value 0.")
        validate_df_processed[col] = 0

    # Reorder columns to match training data
    X_validate_aligned = validate_df_processed[expected_features]
    print(f"âœ… validate_df aligned. Final shape: {X_validate_aligned.shape}")
else:
    print("âŒ X_train not found. Cannot align validate_df.")
    X_validate_aligned = None

# Step 2: Make predictions using the trained model
if X_validate_aligned is not None and 'best_lgbm_model' in locals():
    validate_probs = best_lgbm_model.predict_proba(X_validate_aligned)[:, 1]
    validate_preds = (validate_probs >= OPTIMAL_F2_THRESHOLD).astype(int)

    print("\nğŸ” Validation Predictions Summary:")
    print(f"Total rows: {len(validate_preds)}")
    print(f"Predicted Defaults: {validate_preds.sum()} ({validate_preds.mean()*100:.2f}%)")
    print(f"Predicted Non-Defaults: {len(validate_preds) - validate_preds.sum()}")

    # Step 3: Save Submission
    submission = pd.DataFrame({
        'Customer_ID': testing_data['Customer_ID'],
        'next_month_default': validate_preds
    })
    submission_filename = '23113167csv'  # You may rename this
    submission.to_csv(submission_filename, index=False)
    print(f"\nğŸ“ Submission saved: {submission_filename}")
    print("ğŸ“Œ First 10 Predictions:")
    print(submission.head(10))
else:
    print("âŒ Missing best_lgbm_model or aligned validate data. Skipping prediction and submission.")

# Step 4: Evaluate on holdout/validation set
print("\n" + "="*60)
print("ğŸ“Š MODEL EVALUATION ON X_test")
print("="*60)

if all(name in locals() for name in ['X_test', 'y_test', 'best_lgbm_model']):
    # Predict on holdout set
    test_probs = best_lgbm_model.predict_proba(X_test)[:, 1]
    test_preds = (test_probs >= OPTIMAL_F2_THRESHOLD).astype(int)

    print(f"\nX_test shape: {X_test.shape}")
    print(f"y_test distribution: {y_test.value_counts().to_dict()}")

    # F2 Score and Metrics
    from sklearn.metrics import fbeta_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

    f2 = fbeta_score(y_test, test_preds, beta=2)
    acc = accuracy_score(y_test, test_preds)
    prec = precision_score(y_test, test_preds)
    rec = recall_score(y_test, test_preds)
    f1 = f1_score(y_test, test_preds)

    print("\nğŸ¯ EVALUATION METRICS")
    print(f"F2 Score : {f2:.4f}")
    print(f"Accuracy : {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall   : {rec:.4f}")
    print(f"F1 Score : {f1:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, test_preds)
    print("\nğŸ§® CONFUSION MATRIX")
    print(f"                 Predicted")
    print(f"               0      1")
    print(f"Actual 0    {cm[0,0]:4d}   {cm[0,1]:4d}")
    print(f"       1    {cm[1,0]:4d}   {cm[1,1]:4d}")

    # Class-wise Summary
    print("\nğŸ“‹ CLASSIFICATION REPORT")
    print(classification_report(y_test, test_preds))

    # Save detailed results
    detailed_results = pd.DataFrame({
        'actual': y_test,
        'predicted': test_preds,
        'probability': test_probs
    })
    detailed_results.to_csv('validation_results_detailed.csv', index=False)
    print(f"\nğŸ“ Detailed results saved as: validation_results_detailed.csv")

else:
    print("âŒ Required variables (X_test, y_test, model) not found for evaluation.")

# Final Summary
print("\n" + "="*60)
print("ğŸ“Œ FINAL SUMMARY")
print("="*60)
if X_validate_aligned is not None:
    print(f"âœ… Submission created using threshold: {OPTIMAL_F2_THRESHOLD}")
if 'f2' in locals():
    print(f"âœ… F2 Score on holdout set: {f2:.4f}")
print("ğŸš€ Ready for submission or further analysis!")

import matplotlib.pyplot as plt

# Ensure f2_scores, precisions, recalls, thresholds, best_threshold_f2 are already computed
plt.figure(figsize=(10, 6))

# Plotting curves
plt.plot(thresholds, precisions[:-1], label='ğŸ”µ Precision', color='blue')
plt.plot(thresholds, recalls[:-1], label='ğŸŸ¢ Recall', color='green')
plt.plot(thresholds, f2_scores[:-1], label='ğŸ”´ F2 Score', color='red')

# Highlight best threshold
plt.axvline(best_threshold_f2, color='black', linestyle='--', linewidth=1.5,
            label=f'âš™ï¸ Best Threshold = {best_threshold_f2:.4f}')

# Aesthetic Improvements
plt.title('ğŸ“ˆ LightGBM Performance vs Threshold', fontsize=14, fontweight='bold')
plt.xlabel('Decision Threshold', fontsize=12)
plt.ylabel('Score Value', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

import pandas as pd

print("## ğŸ“Š Summary of Exploratory Data Analysis and Financial Insights\n")

# 1. Target Variable Distribution
print("### ğŸ¯ Target Variable Distribution")
class_counts = training_data['next_month_default'].value_counts()
class_percentages = class_counts / len(training_data) * 100

print(f"- Class Counts:\n{class_counts.to_string()}")
print(f"- Class Percentages:\n{class_percentages.round(2).to_string()}")
print(f"- Insight: The target variable shows imbalance with ~{class_percentages[0]:.2f}% non-defaults (0) and ~{class_percentages[1]:.2f}% defaults (1).\n")

# 2. Credit Limit Analysis
print("### ğŸ’³ Financial Insight - Credit Limit")
median_limit_default = training_data[training_data['next_month_default'] == 1]['LIMIT_BAL'].median()
median_limit_no_default = training_data[training_data['next_month_default'] == 0]['LIMIT_BAL'].median()

print(f"- Median LIMIT_BAL (Defaulters): â‚¹{median_limit_default:,.2f}")
print(f"- Median LIMIT_BAL (Non-Defaulters): â‚¹{median_limit_no_default:,.2f}")
print("- Insight: Defaulters generally have lower credit limits â€” indicating potential risk correlation with limited access to credit.\n")

# 3. Repayment History - PAY_0
print("### ğŸ“… Financial Insight - Repayment History (PAY_0)")
pay0_default = training_data[training_data['next_month_default'] == 1]['pay_0'].value_counts().sort_index()
pay0_non_default = training_data[training_data['next_month_default'] == 0]['pay_0'].value_counts().sort_index()

print("- PAY_0 for Defaulters:")
print(pay0_default.to_string())
print("\n- PAY_0 for Non-Defaulters:")
print(pay0_non_default.to_string())
print("- Insight: Customers with `pay_0 â‰¥ 1` are more frequent in the defaulting group, reflecting missed or late payments as strong predictors of default.\n")

# 4. Credit Utilization
print("### ğŸ“ˆ Financial Insight - Credit Utilization")
median_util_default = training_data[training_data['next_month_default'] == 1]['credit_utilization'].median()
median_util_non_default = training_data[training_data['next_month_default'] == 0]['credit_utilization'].median()

print(f"- Median Utilization (Defaulters): {median_util_default:.2%}")
print(f"- Median Utilization (Non-Defaulters): {median_util_non_default:.2%}")
print("- Insight: Higher utilization (closer to max credit limit) is linked to higher default risk â€” potentially due to financial stress or poor credit behavior.\n")

"""###  Business analysis

"""

from sklearn.metrics import confusion_matrix

def calculate_business_impact(y_true, y_pred, model_name='Model', loan_amount_avg=50000):

    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    # Assumed cost parameters
    cost_of_default = loan_amount_avg * 0.4  # 40% of loan lost on default
    cost_of_investigation = loan_amount_avg * 0.05  # Cost of wrongly rejecting good customer

    # Business costs
    cost_false_negatives = fn * cost_of_default
    cost_false_positives = fp * cost_of_investigation
    total_model_cost = cost_false_negatives + cost_false_positives

    # Baseline cost: No model used
    total_defaults = tp + fn
    baseline_cost = total_defaults * cost_of_default
    savings = baseline_cost - total_model_cost

    # Output results
    print(f"\nğŸ“Š Business Impact Analysis for: {model_name}")
    print("-------------------------------------------------")
    print(f"Confusion Matrix: [TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}]")
    print(f"Total Cost WITH Model:      ${total_model_cost:,.2f}")
    print(f"Total Cost WITHOUT Model:   ${baseline_cost:,.2f}")
    print(f"ğŸ’° Estimated Savings:       ${savings:,.2f}")
    print("-------------------------------------------------")
    print(f"ğŸ”» Cost from False Negatives (missed defaults): ${cost_false_negatives:,.2f}")
    print(f"âš ï¸  Cost from False Positives (unneeded rejects): ${cost_false_positives:,.2f}\n")

    return savings

# Assuming y_test and y_pred_lgbm are defined
savings_lgbm = calculate_business_impact(y_test, y_pred_lgbm, model_name="LightGBM")

